# 언어 모델

## 언어 모델의 개요

### 일반 개념

```{image} figs/image-3-1-1.jpeg
:width: 90%
:align: center
```

- **언어 모델 정의**: 언어 모델은 단어 시퀀스에 확률을 할당하거나 시퀀스에서 다음 단어를 예측하는 계산 모델.
- **적용 분야**: 음성 인식, 기계 번역, 맞춤법 및 문법 수정, 텍스트 생성 등 다양한 자연어 처리 작업에서 중요한 역할.
- **모델 유형**: N-gram 모델(단어 시퀀스에 의존)과 신경 언어 모델(딥러닝 기술을 활용하여 텍스트를 이해하고 생성) 등.
- **중요성**: 언어 모델은 언어의 구조와 패턴을 포착하여 특정 맥락에서 주어진 단어나 구가 나타날 확률을 추정할 수 있음.
- **개발 필요성**: 자연어 처리 시스템의 성능을 개선하기 위해서는 효과적인 언어 모델의 이해와 개발이 필수적.

### 언어 모델이 필요한 이유

```{image} figs/image-3-1-2.jpeg
:width: 90%
:align: center
```

- **모호성 해결**: 언어 모델은 음성 인식과 텍스트 처리에서 다양한 해석에 확률을 할당하고, 맥락을 바탕으로 가장 가능성 높은 해석을 선택하는 데 도움 제공.
- **기계 번역**: 언어 간 텍스트 번역 시, 언어 모델은 목표 언어에서 단어 시퀀스의 유창성과 정확성을 선택하는 데 도움 제공.
- **텍스트 생성**: 언어 모델은 요약, 질문 응답, 대화 시스템과 같은 작업을 위한 일관성 있고 문맥적으로 관련된 텍스트를 생성할 수 있음.
- **맞춤법 및 문법 수정**: 언어 모델은 다양한 단어 시퀀스의 확률을 비교하여 오류를 식별하고 더 가능성 있는 대안을 제안하여 오류를 수정할 수 있음.
- **보조 기술**: 언어 장애가 있는 사용자를 위한 대체 및 보완 커뮤니케이션(AAC) 시스템에서 언어 모델은 사용자에게 가장 가능성이 높은 단어나 구를 예측하고 제안하여 효율적인 커뮤니케이션을 가능하게 함.

## N-gram 언어 모델

### 일반 개념

- **N-gram 언어 모델 정의**: N-gram 언어 모델은 시퀀스에서 다음 단어를 (n-1)개의 이전 단어에 기반하여 예측하는 접근법입니다.
- **N-gram 유형**:
  - **Unigram (단일어)**: 단어 하나만 고려하며, 맥락을 무시합니다 (n=1).
  - **Bigram (이중어)**: 두 단어 시퀀스를 고려합니다 (n=2).
  - **Trigram (삼중어)**: 세 단어 시퀀스를 고려합니다 (n=3).
  - **고차 N-gram**: 더 긴 단어 시퀀스를 고려합니다 (n>3).
- **마르코프 가정**: 단어의 확률은 오직 이전의 (n-1)개 단어에만 의존합니다.
- **확률 추정**: 큰 말뭉치에서 n-gram의 출현 횟수를 세고 정규화하여 확률을 추정합니다.

### N-gram 모델의 장점 및 한계

#### 장점

- 구현이 간단하고 효율적입니다.
- 계산 및 메모리 사용 측면에서 효율적입니다.

#### 한계

- 단어 간 장기 의존성을 포착하지 못합니다.
- 훈련 말뭉치에 나타나지 않은 n-gram에 대해 데이터 희소성 문제에 민감합니다.

### N-gram과 확률 추정

- **확률 추정**: 맥락 h가 주어졌을 때 단어 w의 확률 P(w|h)을 추정할 수 있습니다.
- **확률 추정 방법**: 큰 말뭉치에서 맥락 뒤에 타겟 단어가 나타나는 빈도를 세고 맥락의 총 빈도로 나누어 확률을 추정합니다.

  $$
  P(\text{pizza}|\text{I like to eat}) = \frac{C(\text{I like to eat pizza})}{C(\text{I like to eat})}
  $$

  $$
  P(\text{먹습니다}|\text{저는 김치를}) = \frac{C(\text{저는 김치를 먹습니다})}{C(\text{저는 김치를})}
  $$

#### Bigram 모델

- 전체 맥락을 고려하는 대신 이전 단어만을 고려하여 확률을 추정합니다.
- 이러한 단순화는 확률을 더 신뢰할 수 있게 추정할 수 있게 하지만, 더 긴 맥락 의존성을 포착하지 못할 수 있습니다.

  $$
  P(\text{pizza}|\text{I like to eat}) \approx P(\text{pizza}|\text{eat})
  $$

  $$
  P(\text{먹습니다}|\text{저는 김치를}) \approx P(\text{먹습니다}|\text{김치를})
  $$

### 언어 모델로부터 문장 샘플링

- **문장 샘플링**: 언어 모델로부터 정의된 확률에 따라 문장을 생성하는 방법입니다.
- **과정**: 더 높은 확률을 가진 문장은 낮은 확률을 가진 문장보다 생성될 가능성이 더 높습니다.
- **예시**: Unigram 언어 모델에서는 모델의 어휘에 있는 모든 단어의 확률 분포를 시각화할 수 있으며, Bigram 모델에서는 첫 단어가 `<s>`로 시작하는 랜덤 bigram을 생성합니다.
